{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AlexNet : ImageNet Classification with Deep Convolutional Neural Networks",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch \n",
        "import torchvision \n",
        "import torch.nn as nn \n",
        "import torch.optim as optim \n",
        "import torch.nn.functional as F\n",
        "from PIL import Image\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "FAVl9SAAMa9p"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "QKkRj1mpjJJ3"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.RandomCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "])\n",
        "\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True, \n",
        "    download=True, \n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "validation_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "training_loader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
        "validation_loader = DataLoader(validation_data, batch_size=64, shuffle=True)\n",
        "\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True, \n",
        "    download=True, \n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "validation_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n"
      ],
      "metadata": {
        "id": "a7HpMyUdPd8X"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "IMG SIZE = (W-F+2P)/S + 1\n",
        "'''\n",
        "\n",
        "class AlexNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AlexNet,self).__init__()\n",
        "        self.conv1 = nn.Sequential(\n",
        "            ### SIZE : 1 * 227 * 227\n",
        "            nn.Conv2d(in_channels = 1,out_channels = 96,kernel_size = 11, padding = 0, stride = 4),\n",
        "            ### SIZE : 96 * 55 * 55\n",
        "            nn.ReLU(),\n",
        "            nn.LocalResponseNorm(size=5, k=2),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            ### SIZE : 96 * 27 * 27\n",
        "        )\n",
        "\n",
        "        self.conv2 = nn.Sequential(\n",
        "            ### SIZE : 96 * 27 * 27 \n",
        "            nn.Conv2d(in_channels = 96,out_channels = 256,kernel_size = 5, padding = 2, stride = 1),\n",
        "            ### SIZE = 256 * 27 * 27\n",
        "            nn.ReLU(),\n",
        "            nn.LocalResponseNorm(size=5, k=2),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            ### SIZE : 256 * 13 * 13\n",
        "        )\n",
        "\n",
        "        self.conv3 = nn.Sequential(\n",
        "            ### SIZE : 256 * 13 * 13\n",
        "            nn.Conv2d(in_channels = 256, out_channels = 384, kernel_size = 3,padding = 1, stride = 1),\n",
        "            nn.ReLU(),\n",
        "            ### SIZE : 384 * 13 * 13\n",
        "        )\n",
        "        \n",
        "        self.conv4 = nn.Sequential(\n",
        "            ### SIZE : 384 * 13 * 13\n",
        "            nn.Conv2d(in_channels = 384, out_channels = 384, kernel_size = 3,padding = 1 , stride = 1),\n",
        "            nn.ReLU(),\n",
        "            ### SIZE : 384 * 13 * 13\n",
        "        )\n",
        "\n",
        "        self.conv5 = nn.Sequential(\n",
        "            ### SIZE : 384 * 13 * 13\n",
        "            nn.Conv2d(in_channels = 384, out_channels = 256, kernel_size = 3,padding = 1 , stride = 1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 3, stride = 2)\n",
        "            ### SIZE : 256 * 6 * 6        \n",
        "        )\n",
        "\n",
        "        self.fc1 = nn.Linear( 256 * 6 * 6, 4096 )\n",
        "        self.fc2 = nn.Linear( 4096, 4096 )\n",
        "        self.fc3 = nn.Linear( 4096, 10)\n",
        "        \n",
        "    def forward(self, x:torch.Tensor):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.conv4(x)\n",
        "        x = self.conv5(x)\n",
        "\n",
        "\n",
        "        x = x.view(x.size(0),-1)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x,0.5)\n",
        "        x = self.fc2(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x,0.5)\n",
        "        x = self.fc3(x)\n",
        "        x = F.log_softmax(x,dim=1)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "7yHWgG0QM10v"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "model = AlexNet().to(device)\n",
        "criterion = F.nll_loss # nll_loss : negative log likelihood los\n",
        "optimizer = optim.Adam(model.parameters()) # model(신경망) 파라미터를 optimizer에 전달해줄 때 nn.Module의 parameters() 메소드를 사\n",
        "epochs = 10\n",
        "batch_size = 512"
      ],
      "metadata": {
        "id": "bUSCqed8NInf"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary as summary_\n",
        "\n",
        "\n",
        "summary_(model, (1,227,227), batch_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dz33B4iQjAeU",
        "outputId": "95f84e08-5f03-4ad0-efaa-33b542ba7e46"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1          [512, 96, 55, 55]          11,712\n",
            "              ReLU-2          [512, 96, 55, 55]               0\n",
            " LocalResponseNorm-3          [512, 96, 55, 55]               0\n",
            "         MaxPool2d-4          [512, 96, 27, 27]               0\n",
            "            Conv2d-5         [512, 256, 27, 27]         614,656\n",
            "              ReLU-6         [512, 256, 27, 27]               0\n",
            " LocalResponseNorm-7         [512, 256, 27, 27]               0\n",
            "         MaxPool2d-8         [512, 256, 13, 13]               0\n",
            "            Conv2d-9         [512, 384, 13, 13]         885,120\n",
            "             ReLU-10         [512, 384, 13, 13]               0\n",
            "           Conv2d-11         [512, 384, 13, 13]       1,327,488\n",
            "             ReLU-12         [512, 384, 13, 13]               0\n",
            "           Conv2d-13         [512, 256, 13, 13]         884,992\n",
            "             ReLU-14         [512, 256, 13, 13]               0\n",
            "        MaxPool2d-15           [512, 256, 6, 6]               0\n",
            "           Linear-16                [512, 4096]      37,752,832\n",
            "           Linear-17                [512, 4096]      16,781,312\n",
            "           Linear-18                  [512, 10]          40,970\n",
            "================================================================\n",
            "Total params: 58,299,082\n",
            "Trainable params: 58,299,082\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 100.64\n",
            "Forward/backward pass size (MB): 7452.54\n",
            "Params size (MB): 222.39\n",
            "Estimated Total Size (MB): 7775.57\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        # enumberate() : 인덱스와 원소로 이루어진 튜플(tuple)을 만들어줌\n",
        "        target = target.type(torch.LongTensor)\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad() # 항상 backpropagation 하기전에 미분(gradient)을 zero로 만들어주고 시작해야 한다.\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target) # criterion = loss_fn\n",
        "        loss.backward() # Computes the gradient of current tensor w.r.t. graph leaves\n",
        "        optimizer.step() # step() : 파라미터를 업데이트함\n",
        "        if (batch_idx + 1) % 30 == 0:\n",
        "            print(\"Train Epoch:{} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))"
      ],
      "metadata": {
        "id": "0y2-uHmNmZmE"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += criterion(output, target, reduction='sum').item()\n",
        "            pred = output.max(1, keepdim=True)[1]\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "        test_loss /= len(test_loader.dataset)  # -> mean\n",
        "        print(\"\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n\".format(\n",
        "            test_loss, correct, len(test_loader.dataset), 100. * correct / len(test_loader.dataset)))\n",
        "        print('='*50)"
      ],
      "metadata": {
        "id": "TX3E33D1rXc3"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1, epochs+1):\n",
        "    train(model, device, training_loader, optimizer, epoch)\n",
        "    test(model, device, validation_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GbjTbVI6rYN2",
        "outputId": "56007f75-8f76-4345-8e9d-30bef1f7f7be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch:1 [1856/60000 (3%)]\tLoss: 1.065234\n",
            "Train Epoch:1 [3776/60000 (6%)]\tLoss: 0.795542\n",
            "Train Epoch:1 [5696/60000 (9%)]\tLoss: 0.743207\n",
            "Train Epoch:1 [7616/60000 (13%)]\tLoss: 0.789146\n",
            "Train Epoch:1 [9536/60000 (16%)]\tLoss: 0.502621\n",
            "Train Epoch:1 [11456/60000 (19%)]\tLoss: 0.443591\n",
            "Train Epoch:1 [13376/60000 (22%)]\tLoss: 0.449500\n",
            "Train Epoch:1 [15296/60000 (25%)]\tLoss: 0.652632\n",
            "Train Epoch:1 [17216/60000 (29%)]\tLoss: 0.432769\n",
            "Train Epoch:1 [19136/60000 (32%)]\tLoss: 0.553776\n",
            "Train Epoch:1 [21056/60000 (35%)]\tLoss: 0.589020\n",
            "Train Epoch:1 [22976/60000 (38%)]\tLoss: 0.364813\n",
            "Train Epoch:1 [24896/60000 (41%)]\tLoss: 0.343011\n",
            "Train Epoch:1 [26816/60000 (45%)]\tLoss: 0.381429\n",
            "Train Epoch:1 [28736/60000 (48%)]\tLoss: 0.201259\n",
            "Train Epoch:1 [30656/60000 (51%)]\tLoss: 0.458894\n",
            "Train Epoch:1 [32576/60000 (54%)]\tLoss: 0.495144\n",
            "Train Epoch:1 [34496/60000 (57%)]\tLoss: 0.388077\n",
            "Train Epoch:1 [36416/60000 (61%)]\tLoss: 0.274467\n",
            "Train Epoch:1 [38336/60000 (64%)]\tLoss: 0.294548\n",
            "Train Epoch:1 [40256/60000 (67%)]\tLoss: 0.457301\n",
            "Train Epoch:1 [42176/60000 (70%)]\tLoss: 0.306421\n",
            "Train Epoch:1 [44096/60000 (73%)]\tLoss: 0.405102\n",
            "Train Epoch:1 [46016/60000 (77%)]\tLoss: 0.416696\n",
            "Train Epoch:1 [47936/60000 (80%)]\tLoss: 0.287350\n",
            "Train Epoch:1 [49856/60000 (83%)]\tLoss: 0.244187\n",
            "Train Epoch:1 [51776/60000 (86%)]\tLoss: 0.434800\n",
            "Train Epoch:1 [53696/60000 (89%)]\tLoss: 0.330205\n",
            "Train Epoch:1 [55616/60000 (93%)]\tLoss: 0.440987\n",
            "Train Epoch:1 [57536/60000 (96%)]\tLoss: 0.306927\n",
            "Train Epoch:1 [59456/60000 (99%)]\tLoss: 0.326899\n",
            "\n",
            "Test set: Average loss: 0.3677, Accuracy: 8673/10000 (87%)\n",
            "\n",
            "==================================================\n",
            "Train Epoch:2 [1856/60000 (3%)]\tLoss: 0.465772\n",
            "Train Epoch:2 [3776/60000 (6%)]\tLoss: 0.274527\n",
            "Train Epoch:2 [5696/60000 (9%)]\tLoss: 0.261022\n",
            "Train Epoch:2 [7616/60000 (13%)]\tLoss: 0.239124\n",
            "Train Epoch:2 [9536/60000 (16%)]\tLoss: 0.333463\n",
            "Train Epoch:2 [11456/60000 (19%)]\tLoss: 0.232262\n",
            "Train Epoch:2 [13376/60000 (22%)]\tLoss: 0.348995\n",
            "Train Epoch:2 [15296/60000 (25%)]\tLoss: 0.428223\n",
            "Train Epoch:2 [17216/60000 (29%)]\tLoss: 0.257616\n",
            "Train Epoch:2 [19136/60000 (32%)]\tLoss: 0.348409\n",
            "Train Epoch:2 [21056/60000 (35%)]\tLoss: 0.312354\n",
            "Train Epoch:2 [22976/60000 (38%)]\tLoss: 0.387666\n",
            "Train Epoch:2 [24896/60000 (41%)]\tLoss: 0.490645\n",
            "Train Epoch:2 [26816/60000 (45%)]\tLoss: 0.181117\n",
            "Train Epoch:2 [28736/60000 (48%)]\tLoss: 0.272147\n",
            "Train Epoch:2 [30656/60000 (51%)]\tLoss: 0.316225\n",
            "Train Epoch:2 [32576/60000 (54%)]\tLoss: 0.399654\n",
            "Train Epoch:2 [34496/60000 (57%)]\tLoss: 0.200939\n",
            "Train Epoch:2 [36416/60000 (61%)]\tLoss: 0.231398\n",
            "Train Epoch:2 [38336/60000 (64%)]\tLoss: 0.301173\n",
            "Train Epoch:2 [40256/60000 (67%)]\tLoss: 0.151791\n",
            "Train Epoch:2 [42176/60000 (70%)]\tLoss: 0.237229\n",
            "Train Epoch:2 [44096/60000 (73%)]\tLoss: 0.263986\n",
            "Train Epoch:2 [46016/60000 (77%)]\tLoss: 0.292443\n",
            "Train Epoch:2 [47936/60000 (80%)]\tLoss: 0.411207\n",
            "Train Epoch:2 [49856/60000 (83%)]\tLoss: 0.372938\n",
            "Train Epoch:2 [51776/60000 (86%)]\tLoss: 0.329592\n",
            "Train Epoch:2 [53696/60000 (89%)]\tLoss: 0.275448\n",
            "Train Epoch:2 [55616/60000 (93%)]\tLoss: 0.213471\n",
            "Train Epoch:2 [57536/60000 (96%)]\tLoss: 0.221395\n",
            "Train Epoch:2 [59456/60000 (99%)]\tLoss: 0.243175\n",
            "\n",
            "Test set: Average loss: 0.3105, Accuracy: 8871/10000 (89%)\n",
            "\n",
            "==================================================\n",
            "Train Epoch:3 [1856/60000 (3%)]\tLoss: 0.404416\n",
            "Train Epoch:3 [3776/60000 (6%)]\tLoss: 0.389350\n",
            "Train Epoch:3 [5696/60000 (9%)]\tLoss: 0.247180\n",
            "Train Epoch:3 [7616/60000 (13%)]\tLoss: 0.173021\n",
            "Train Epoch:3 [9536/60000 (16%)]\tLoss: 0.117481\n"
          ]
        }
      ]
    }
  ]
}