{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2. Batch_Normalization_Experiment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMhSq6xPCwZUNB9k+EPy7x7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kyj098707/Deep-Learning-Paeper-Review-and-Code/blob/master/2_Batch_Normalization_Experiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pmUbQQXJU4eN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torchvision.datasets\n",
        "import torchvision.transforms as transforms\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "learning_rate = 0.001\n",
        "training_epochs = 15\n",
        "batch_size = 100"
      ],
      "metadata": {
        "id": "eOFQQMzYVIAK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mnist_train = torchvision.datasets.MNIST(root='MNIST_data/',\n",
        "                          train=True,\n",
        "                          transform=transforms.ToTensor(),\n",
        "                          download=True)\n",
        "\n",
        "mnist_test = torchvision.datasets.MNIST(root='MNIST_data/',\n",
        "                         train=False,\n",
        "                         transform=transforms.ToTensor(),\n",
        "                         download=True)"
      ],
      "metadata": {
        "id": "gfjtOgT0VRdh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(dataset=mnist_train,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=True,\n",
        "                                          drop_last=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=mnist_test,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=False,\n",
        "                                          drop_last=True)"
      ],
      "metadata": {
        "id": "rNaURcFKVTtN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 배치 정규화 적용 x\n",
        "without_BN_model = torch.nn.Sequential(\n",
        "    nn.Linear(784,32,bias=True),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(32,32,bias=True),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(32,10,bias=True),\n",
        ").to(device)\n",
        "\n",
        "##  배치 정규화 적용 o\n",
        "with_BN_model = torch.nn.Sequential(\n",
        "    nn.Linear(784,32,bias=True),\n",
        "    nn.BatchNorm1d(32),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(32,32,bias=True),\n",
        "    nn.BatchNorm1d(32),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(32,10,bias=True),\n",
        ").to(device)"
      ],
      "metadata": {
        "id": "UZvUBOvVVcmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
        "BN_optimizer = torch.optim.Adam(with_BN_model.parameters(), lr=learning_rate)\n",
        "WOBN_optimizer = torch.optim.Adam(without_BN_model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "xN2fPusqYBTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses = []\n",
        "train_accs = []\n",
        "\n",
        "valid_losses = []\n",
        "valid_accs = []\n",
        "\n",
        "train_total_batch = len(train_loader)\n",
        "test_total_batch = len(test_loader)\n",
        "for epoch in range(training_epochs):\n",
        "    with_BN_model.train()\n",
        "\n",
        "    for X, Y in train_loader:\n",
        "        X = X.view(-1, 28 * 28).to(device)\n",
        "        Y = Y.to(device)\n",
        "\n",
        "        BN_optimizer.zero_grad()\n",
        "        bn_prediction = with_BN_model(X)\n",
        "        bn_loss = criterion(bn_prediction, Y)\n",
        "        bn_loss.backward()\n",
        "        BN_optimizer.step()\n",
        "\n",
        "        WOBN_optimizer.zero_grad()\n",
        "        wobn_prediction = without_BN_model(X)\n",
        "        wobn_loss = criterion(wobn_prediction, Y)\n",
        "        wobn_loss.backward()\n",
        "        WOBN_optimizer.step()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        with_BN_model.eval()     # set the model to evaluation mode\n",
        "\n",
        "        # Test the model using train sets\n",
        "        bn_loss, wobn_loss, bn_acc, wobn_acc = 0, 0, 0, 0\n",
        "        for i, (X, Y) in enumerate(train_loader):\n",
        "            X = X.view(-1, 28 * 28).to(device)\n",
        "            Y = Y.to(device)\n",
        "\n",
        "            bn_prediction = with_BN_model(X)\n",
        "            bn_correct_prediction = torch.argmax(bn_prediction, 1) == Y\n",
        "            bn_loss += criterion(bn_prediction, Y)\n",
        "            bn_acc += bn_correct_prediction.float().mean()\n",
        "\n",
        "            wobn_prediction = without_BN_model(X)\n",
        "            wobn_correct_prediction = torch.argmax(wobn_prediction, 1) == Y\n",
        "            wobn_loss += criterion(wobn_prediction, Y)\n",
        "            wobn_acc += wobn_correct_prediction.float().mean()\n",
        "\n",
        "        bn_loss, wobn_loss, bn_acc, wobn_acc = bn_loss / train_total_batch, wobn_loss / train_total_batch, bn_acc / train_total_batch, wobn_acc / train_total_batch\n",
        "\n",
        "        # Save train losses/acc\n",
        "        train_losses.append([bn_loss, wobn_loss])\n",
        "        train_accs.append([bn_acc, wobn_acc])\n",
        "        print(\n",
        "            '[Epoch %d-TRAIN] Batchnorm Loss(Acc): bn_loss:%.5f(bn_acc:%.2f) vs No Batchnorm Loss(Acc): wobn_loss:%.5f(wobn_acc:%.2f)' % (\n",
        "            (epoch + 1), bn_loss.item(), bn_acc.item(), wobn_loss.item(), wobn_acc.item()))\n",
        "        # Test the model using test sets\n",
        "        bn_loss, wobn_loss, bn_acc, wobn_acc = 0, 0, 0, 0\n",
        "        for i, (X, Y) in enumerate(test_loader):\n",
        "            X = X.view(-1, 28 * 28).to(device)\n",
        "            Y = Y.to(device)\n",
        "\n",
        "            bn_prediction = with_BN_model(X)\n",
        "            bn_correct_prediction = torch.argmax(bn_prediction, 1) == Y\n",
        "            bn_loss += criterion(bn_prediction, Y)\n",
        "            bn_acc += bn_correct_prediction.float().mean()\n",
        "\n",
        "            wobn_prediction = without_BN_model(X)\n",
        "            wobn_correct_prediction = torch.argmax(wobn_prediction, 1) == Y\n",
        "            wobn_loss += criterion(wobn_prediction, Y)\n",
        "            wobn_acc += wobn_correct_prediction.float().mean()\n",
        "\n",
        "        bn_loss, wobn_loss, bn_acc, wobn_acc = bn_loss / test_total_batch, wobn_loss / test_total_batch, bn_acc / test_total_batch, wobn_acc / test_total_batch\n",
        "\n",
        "        # Save valid losses/acc\n",
        "        valid_losses.append([bn_loss, wobn_loss])\n",
        "        valid_accs.append([bn_acc, wobn_acc])\n",
        "        print(\n",
        "            '[Epoch %d-VALID] Batchnorm Loss(Acc): bn_loss:%.5f(bn_acc:%.2f) vs No Batchnorm Loss(Acc): wobn_loss:%.5f(wobn_acc:%.2f)' % (\n",
        "                (epoch + 1), bn_loss.item(), bn_acc.item(), wobn_loss.item(), wobn_acc.item()))\n",
        "        print()\n",
        "\n",
        "print('Learning finished')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oy4PqD2-VKJS",
        "outputId": "470c612b-9df5-4195-d47d-5bd7d4596cf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1-TRAIN] Batchnorm Loss(Acc): bn_loss:0.16300(bn_acc:0.96) vs No Batchnorm Loss(Acc): wobn_loss:0.26204(wobn_acc:0.92)\n",
            "[Epoch 1-VALID] Batchnorm Loss(Acc): bn_loss:0.16806(bn_acc:0.95) vs No Batchnorm Loss(Acc): wobn_loss:0.25685(wobn_acc:0.93)\n",
            "\n",
            "[Epoch 2-TRAIN] Batchnorm Loss(Acc): bn_loss:0.10865(bn_acc:0.97) vs No Batchnorm Loss(Acc): wobn_loss:0.19944(wobn_acc:0.94)\n",
            "[Epoch 2-VALID] Batchnorm Loss(Acc): bn_loss:0.12802(bn_acc:0.96) vs No Batchnorm Loss(Acc): wobn_loss:0.20185(wobn_acc:0.94)\n",
            "\n",
            "[Epoch 3-TRAIN] Batchnorm Loss(Acc): bn_loss:0.08525(bn_acc:0.98) vs No Batchnorm Loss(Acc): wobn_loss:0.16570(wobn_acc:0.95)\n",
            "[Epoch 3-VALID] Batchnorm Loss(Acc): bn_loss:0.10932(bn_acc:0.97) vs No Batchnorm Loss(Acc): wobn_loss:0.17320(wobn_acc:0.95)\n",
            "\n",
            "[Epoch 4-TRAIN] Batchnorm Loss(Acc): bn_loss:0.07119(bn_acc:0.98) vs No Batchnorm Loss(Acc): wobn_loss:0.14366(wobn_acc:0.96)\n",
            "[Epoch 4-VALID] Batchnorm Loss(Acc): bn_loss:0.10382(bn_acc:0.97) vs No Batchnorm Loss(Acc): wobn_loss:0.15683(wobn_acc:0.95)\n",
            "\n",
            "[Epoch 5-TRAIN] Batchnorm Loss(Acc): bn_loss:0.06313(bn_acc:0.98) vs No Batchnorm Loss(Acc): wobn_loss:0.12200(wobn_acc:0.96)\n",
            "[Epoch 5-VALID] Batchnorm Loss(Acc): bn_loss:0.10638(bn_acc:0.97) vs No Batchnorm Loss(Acc): wobn_loss:0.13939(wobn_acc:0.96)\n",
            "\n",
            "[Epoch 6-TRAIN] Batchnorm Loss(Acc): bn_loss:0.05430(bn_acc:0.98) vs No Batchnorm Loss(Acc): wobn_loss:0.10611(wobn_acc:0.97)\n",
            "[Epoch 6-VALID] Batchnorm Loss(Acc): bn_loss:0.09697(bn_acc:0.97) vs No Batchnorm Loss(Acc): wobn_loss:0.12831(wobn_acc:0.96)\n",
            "\n",
            "[Epoch 7-TRAIN] Batchnorm Loss(Acc): bn_loss:0.05087(bn_acc:0.98) vs No Batchnorm Loss(Acc): wobn_loss:0.09370(wobn_acc:0.97)\n",
            "[Epoch 7-VALID] Batchnorm Loss(Acc): bn_loss:0.10214(bn_acc:0.97) vs No Batchnorm Loss(Acc): wobn_loss:0.11971(wobn_acc:0.96)\n",
            "\n",
            "[Epoch 8-TRAIN] Batchnorm Loss(Acc): bn_loss:0.04746(bn_acc:0.99) vs No Batchnorm Loss(Acc): wobn_loss:0.08774(wobn_acc:0.97)\n",
            "[Epoch 8-VALID] Batchnorm Loss(Acc): bn_loss:0.09741(bn_acc:0.97) vs No Batchnorm Loss(Acc): wobn_loss:0.11524(wobn_acc:0.97)\n",
            "\n",
            "[Epoch 9-TRAIN] Batchnorm Loss(Acc): bn_loss:0.03933(bn_acc:0.99) vs No Batchnorm Loss(Acc): wobn_loss:0.08330(wobn_acc:0.97)\n",
            "[Epoch 9-VALID] Batchnorm Loss(Acc): bn_loss:0.09725(bn_acc:0.97) vs No Batchnorm Loss(Acc): wobn_loss:0.11754(wobn_acc:0.96)\n",
            "\n",
            "[Epoch 10-TRAIN] Batchnorm Loss(Acc): bn_loss:0.03935(bn_acc:0.99) vs No Batchnorm Loss(Acc): wobn_loss:0.06990(wobn_acc:0.98)\n",
            "[Epoch 10-VALID] Batchnorm Loss(Acc): bn_loss:0.10215(bn_acc:0.97) vs No Batchnorm Loss(Acc): wobn_loss:0.10486(wobn_acc:0.97)\n",
            "\n",
            "[Epoch 11-TRAIN] Batchnorm Loss(Acc): bn_loss:0.03556(bn_acc:0.99) vs No Batchnorm Loss(Acc): wobn_loss:0.07086(wobn_acc:0.98)\n",
            "[Epoch 11-VALID] Batchnorm Loss(Acc): bn_loss:0.09996(bn_acc:0.97) vs No Batchnorm Loss(Acc): wobn_loss:0.11220(wobn_acc:0.97)\n",
            "\n",
            "[Epoch 12-TRAIN] Batchnorm Loss(Acc): bn_loss:0.03439(bn_acc:0.99) vs No Batchnorm Loss(Acc): wobn_loss:0.06344(wobn_acc:0.98)\n",
            "[Epoch 12-VALID] Batchnorm Loss(Acc): bn_loss:0.09634(bn_acc:0.97) vs No Batchnorm Loss(Acc): wobn_loss:0.10916(wobn_acc:0.97)\n",
            "\n",
            "[Epoch 13-TRAIN] Batchnorm Loss(Acc): bn_loss:0.03054(bn_acc:0.99) vs No Batchnorm Loss(Acc): wobn_loss:0.05581(wobn_acc:0.98)\n",
            "[Epoch 13-VALID] Batchnorm Loss(Acc): bn_loss:0.10166(bn_acc:0.97) vs No Batchnorm Loss(Acc): wobn_loss:0.10176(wobn_acc:0.97)\n",
            "\n",
            "[Epoch 14-TRAIN] Batchnorm Loss(Acc): bn_loss:0.02789(bn_acc:0.99) vs No Batchnorm Loss(Acc): wobn_loss:0.05101(wobn_acc:0.99)\n",
            "[Epoch 14-VALID] Batchnorm Loss(Acc): bn_loss:0.09739(bn_acc:0.97) vs No Batchnorm Loss(Acc): wobn_loss:0.10355(wobn_acc:0.97)\n",
            "\n",
            "[Epoch 15-TRAIN] Batchnorm Loss(Acc): bn_loss:0.02609(bn_acc:0.99) vs No Batchnorm Loss(Acc): wobn_loss:0.04840(wobn_acc:0.99)\n",
            "[Epoch 15-VALID] Batchnorm Loss(Acc): bn_loss:0.09352(bn_acc:0.97) vs No Batchnorm Loss(Acc): wobn_loss:0.10731(wobn_acc:0.97)\n",
            "\n",
            "Learning finished\n"
          ]
        }
      ]
    }
  ]
}